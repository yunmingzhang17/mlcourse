\section{Problem 3}

\subsection{Part1}

{\bfseries Algorithm} \\
We first create a centered x data matrix Z. And then we cerate a centered y values.

\begin{equation}
z^{(i)}_{j} = x^{(i)}_{j} - \bar{x}_{j} \hspace{10 mm} 
y^{(i)}_{c} = y^{(i)} - \bar{y} \\
\end{equation}

Then we use the formula for calculating the error. It uses a $\lambda$ for regularizing the
weight terms. 

\begin{equation}
  E(w)= (Y_{c} - ZW)^T(Y_{c}-ZW) + \lambda W^TW \\
\end{equation}

We used a regularized term to calculate optimal weight using the following equation

\begin{equation}
  W_{ridge} = (Z^{T}Z + \lambda I)^{-1}Z^{T}Y_{c}
\end{equation}

{\bfseries Experimentation} \\
We have conducted a series of experimentation with the simple data from Bishop's Figure 1.4

%TODO: generate graphs to show the impact of increasing M (overfitting), but this is shown in the previou squestion

%TODO: generate graphs to show the inmpact of increaing lambda




\subsection{Part2}


%TODO: generate graphs to show the impact of increasing M (overfitting)

%TODO: generate graphs to show the inmpact of increaing lambda

%TODO: show that the models trained from data A is a lot better than model B


\subsection{Part3}
